{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58213cf0",
   "metadata": {},
   "source": [
    "# ğŸ” Fake News Detection with Explainable NLP\n",
    "\n",
    "This notebook demonstrates the complete pipeline for detecting fake news with interpretable AI explanations.\n",
    "\n",
    "## Features:\n",
    "- Text preprocessing and cleaning\n",
    "- TF-IDF feature extraction\n",
    "- Multiple ML models (Logistic Regression, Random Forest, SVM)\n",
    "- LIME explanations for predictions\n",
    "- Visualization of word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c46eda",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae57346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy scikit-learn nltk lime shap matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root and src directory to path\n",
    "# When running from notebooks folder, go up one level\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import our modules (ignore Pylance warnings - these resolve at runtime)\n",
    "from preprocessing import TextPreprocessor, DataLoader, create_sample_dataset  # type: ignore\n",
    "from feature_engineering import TfidfFeatureExtractor  # type: ignore\n",
    "from models import ModelTrainer, ModelComparison, print_evaluation_report  # type: ignore\n",
    "from explainability import LimeExplainer, TextExplainerPipeline, visualize_word_importance  # type: ignore\n",
    "\n",
    "print(f\"âœ… Project root: {project_root}\")\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d5f37",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "We'll create a sample dataset for demonstration. Replace this with your actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06099005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for demo\n",
    "os.makedirs('data', exist_ok=True)\n",
    "create_sample_dataset('data/sample_news.csv', num_samples=1000)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/sample_news.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"Label Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df['label'].value_counts().plot(kind='bar', ax=ax, color=['#e74c3c', '#2ecc71'])\n",
    "ax.set_title('Label Distribution', fontsize=14)\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5df167",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fe708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Example preprocessing\n",
    "sample_text = \"BREAKING NEWS!!! Scientists discover SHOCKING evidence at https://example.com ğŸ˜±ğŸ˜±ğŸ˜±\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Preprocessed: {preprocessor.preprocess(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all texts\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].map({'FAKE': 0, 'REAL': 1}).values\n",
    "\n",
    "preprocessed_texts = preprocessor.preprocess_batch(texts, verbose=False)\n",
    "print(f\"Preprocessed {len(preprocessed_texts)} texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffff98",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e36cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    preprocessed_texts, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb197df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfFeatureExtractor(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top features\n",
    "top_features = tfidf.get_top_features(20)\n",
    "print(\"Top 20 TF-IDF features (by IDF score):\")\n",
    "for word, score in top_features:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36594241",
   "metadata": {},
   "source": [
    "## 5. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "comparison = ModelComparison(\n",
    "    models=['logistic_regression', 'random_forest', 'svm', 'naive_bayes'],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "results = comparison.compare(X_train_tfidf, y_train)\n",
    "print(\"\\n\" + comparison.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb220df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_type = comparison.get_best_model('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "model_names = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for model, result in results.items():\n",
    "    if 'accuracy' in result:\n",
    "        model_names.append(model)\n",
    "        accuracies.append(result['accuracy']['mean'])\n",
    "        f1_scores.append(result['f1']['mean'])\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', color='#2ecc71')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace('_', ' ').title() for m in model_names], rotation=45)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4053a8",
   "metadata": {},
   "source": [
    "## 6. Train Final Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0750748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "model = ModelTrainer('logistic_regression', max_iter=1000, C=1.0)\n",
    "model.train(X_train_tfidf, y_train, feature_names=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31380b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = model.evaluate(X_test_tfidf, y_test, class_names=['Fake', 'Real'])\n",
    "print_evaluation_report(metrics, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    model.predict(X_test_tfidf),\n",
    "    display_labels=['Fake', 'Real'],\n",
    "    cmap='Blues',\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ee0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = model.get_feature_importance(top_n=20)\n",
    "\n",
    "if feature_importance:\n",
    "    words = [w for w, _ in feature_importance]\n",
    "    scores = [s for _, s in feature_importance]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    y_pos = np.arange(len(words))\n",
    "    ax.barh(y_pos, scores, color='#3498db', alpha=0.8)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(words)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Importance Score')\n",
    "    ax.set_title('Top 20 Most Important Features', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9429f5",
   "metadata": {},
   "source": [
    "## 7. Explainability with LIME ğŸ”\n",
    "\n",
    "LIME (Local Interpretable Model-Agnostic Explanations) helps us understand **why** the model made a specific prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91da113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainer pipeline\n",
    "explainer_pipeline = TextExplainerPipeline(\n",
    "    model=model.model,\n",
    "    vectorizer=tfidf.vectorizer,\n",
    "    preprocessor=preprocessor,\n",
    "    class_names=['Fake', 'Real']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explain a fake news sample\n",
    "fake_news_example = \"BREAKING: Government hides shocking conspiracy from public! You won't believe what was revealed!\"\n",
    "\n",
    "result = explainer_pipeline.predict_and_explain(fake_news_example, num_features=10)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FAKE NEWS DETECTION RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nText: {fake_news_example}\")\n",
    "print(f\"\\nğŸ“Š Prediction: {result['predicted_class']}\")\n",
    "print(f\"ğŸ“ˆ Confidence: {result['confidence']:.2%}\")\n",
    "print(\"\\nğŸ” Why this prediction? (Word Importance):\")\n",
    "for word, score in result['word_importance']:\n",
    "    direction = \"â†’ Real\" if score > 0 else \"â†’ Fake\"\n",
    "    print(f\"   '{word}': {score:+.4f} {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word importance\n",
    "visualize_word_importance(\n",
    "    result['word_importance'],\n",
    "    title=f\"Word Importance for Prediction: {result['predicted_class']}\",\n",
    "    figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explain a real news sample\n",
    "real_news_example = \"Research study indicates that climate changes may affect economic growth rates over the next decade.\"\n",
    "\n",
    "result2 = explainer_pipeline.predict_and_explain(real_news_example, num_features=10)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REAL NEWS DETECTION RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nText: {real_news_example}\")\n",
    "print(f\"\\nğŸ“Š Prediction: {result2['predicted_class']}\")\n",
    "print(f\"ğŸ“ˆ Confidence: {result2['confidence']:.2%}\")\n",
    "print(\"\\nğŸ” Why this prediction? (Word Importance):\")\n",
    "for word, score in result2['word_importance']:\n",
    "    direction = \"â†’ Real\" if score > 0 else \"â†’ Fake\"\n",
    "    print(f\"   '{word}': {score:+.4f} {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real news explanation\n",
    "visualize_word_importance(\n",
    "    result2['word_importance'],\n",
    "    title=f\"Word Importance for Prediction: {result2['predicted_class']}\",\n",
    "    figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3269101",
   "metadata": {},
   "source": [
    "## 8. Interactive Classification\n",
    "\n",
    "Try classifying your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_explain(text):\n",
    "    \"\"\"Classify text and show explanation.\"\"\"\n",
    "    result = explainer_pipeline.predict_and_explain(text, num_features=8)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“° INPUT: {text[:100]}...\" if len(text) > 100 else f\"ğŸ“° INPUT: {text}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prediction with emoji\n",
    "    emoji = \"âŒ\" if result['predicted_class'] == 'Fake' else \"âœ…\"\n",
    "    print(f\"\\n{emoji} Prediction: {result['predicted_class']}\")\n",
    "    print(f\"ğŸ“Š Confidence: {result['confidence']:.1%}\")\n",
    "    \n",
    "    # Probability bar\n",
    "    fake_prob = result['probabilities']['Fake']\n",
    "    real_prob = result['probabilities']['Real']\n",
    "    print(f\"\\n   Fake: {'â–ˆ' * int(fake_prob * 20):<20} {fake_prob:.1%}\")\n",
    "    print(f\"   Real: {'â–ˆ' * int(real_prob * 20):<20} {real_prob:.1%}\")\n",
    "    \n",
    "    # Word importance\n",
    "    print(\"\\nğŸ” Key Influential Words:\")\n",
    "    for word, score in result['word_importance'][:5]:\n",
    "        bar = 'ğŸŸ¢' if score > 0 else 'ğŸ”´'\n",
    "        print(f\"   {bar} {word}: {score:+.3f}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different examples\n",
    "test_texts = [\n",
    "    \"Scientists at MIT publish groundbreaking research on renewable energy solutions.\",\n",
    "    \"SHOCKING: This one weird trick will change everything you know about health!\",\n",
    "    \"The Federal Reserve announced a 0.25% interest rate increase citing inflation concerns.\",\n",
    "    \"You won't BELIEVE what celebrities are hiding from the public!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    classify_and_explain(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecea7c9",
   "metadata": {},
   "source": [
    "## 9. Save Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and vectorizer\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "model.save('models/fake_news_model.pkl')\n",
    "tfidf.save('models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"âœ… Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5558dba",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. âœ… Loaded and preprocessed text data\n",
    "2. âœ… Extracted TF-IDF features\n",
    "3. âœ… Compared multiple ML models\n",
    "4. âœ… Trained and evaluated the best model\n",
    "5. âœ… Implemented LIME explanations\n",
    "6. âœ… Visualized feature importance\n",
    "\n",
    "### Future Improvements:\n",
    "- ğŸš€ Add BERT embeddings for better accuracy\n",
    "- ğŸŒ Implement multilingual support\n",
    "- ğŸ“± Build web API for real-time detection\n",
    "- ğŸ”„ Add active learning for continuous improvement"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
